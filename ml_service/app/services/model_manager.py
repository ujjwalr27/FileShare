"""
Model Manager for lazy loading ML models.
Only one model is loaded at a time to optimize memory usage on systems with 8GB RAM.
"""
from __future__ import annotations

import gc
from typing import Optional, Literal, TYPE_CHECKING

# Lazy imports - these are slow and block server startup
# Only import type hints for IDE support
if TYPE_CHECKING:
    from sentence_transformers import SentenceTransformer
    import spacy

ModelType = Literal["semantic_search", "pii_detection"]

class ModelManager:
    """
    Manages ML models with lazy loading and memory optimization.
    Only loads one model at a time to stay within 8GB RAM constraint.
    """

    def __init__(self):
        self.current_model: Optional[str] = None
        self.semantic_model = None  # Lazy: SentenceTransformer
        self.pii_model = None  # Lazy: spacy.Language

    def _unload_all_models(self):
        """Unload all models to free memory."""
        if self.semantic_model is not None:
            del self.semantic_model
            self.semantic_model = None

        if self.pii_model is not None:
            del self.pii_model
            self.pii_model = None

        # Force garbage collection
        gc.collect()

        # Clear PyTorch cache if available (lazy import)
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except ImportError:
            pass

        self.current_model = None

    def get_semantic_model(self):
        """
        Get the semantic search model (MiniLM-L3-v2).
        Unloads other models if necessary.
        """
        if self.current_model != "semantic_search":
            print("üì• Loading semantic search model...")
            self._unload_all_models()

            try:
                # Load lightweight sentence transformer model (~80MB)
                # Set cache_folder and use local files if available
                import os
                cache_dir = os.path.join(os.getcwd(), 'models', 'sentence-transformers')
                os.makedirs(cache_dir, exist_ok=True)
                
                print(f"Using cache directory: {cache_dir}")
                
                # Lazy import - only load when needed
                from sentence_transformers import SentenceTransformer
                
                # Use the more common and stable all-MiniLM-L6-v2 model
                self.semantic_model = SentenceTransformer(
                    'all-MiniLM-L6-v2',
                    device='cpu',  # Force CPU usage
                    cache_folder=cache_dir
                )
                self.current_model = "semantic_search"
                print("‚úÖ Semantic search model loaded (all-MiniLM-L6-v2)")
            except Exception as e:
                print(f"‚ùå Error loading semantic search model: {str(e)}")
                raise Exception(
                    f"Failed to load semantic search model. "
                    f"Please run 'python download_models.py' first to download required models. "
                    f"Original error: {str(e)}"
                )

        return self.semantic_model

    def get_pii_model(self):
        """
        Get the PII detection model (spaCy).
        Unloads other models if necessary.
        """
        if self.current_model != "pii_detection":
            print("üì• Loading PII detection model...")
            self._unload_all_models()

            try:
                # Lazy import spacy
                import spacy
                # Try to load the model if it's already downloaded
                self.pii_model = spacy.load("en_core_web_sm")
            except OSError:
                # If not downloaded, download it first
                print("üì¶ Downloading spaCy model (one-time only)...")
                import subprocess
                subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])
                import spacy
                self.pii_model = spacy.load("en_core_web_sm")

            self.current_model = "pii_detection"
            print("‚úÖ PII detection model loaded")

        return self.pii_model

    def cleanup(self):
        """Clean up all models and free memory."""
        print("üßπ Cleaning up models...")
        self._unload_all_models()
        print("‚úÖ Cleanup complete")
